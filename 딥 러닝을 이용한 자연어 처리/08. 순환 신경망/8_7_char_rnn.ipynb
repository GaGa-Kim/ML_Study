{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 문자 단위 RNN 언어 모델"
      ],
      "metadata": {
        "id": "08IPJ8QMe7YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문자 단위 RNN 언어 모델 구현하기 (다 대 다 LSTM)"
      ],
      "metadata": {
        "id": "dqdh625Ae8rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) 데이터에 대한 이해와 전처리**"
      ],
      "metadata": {
        "id": "HJMDxy7ze9t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "FQUH0hPJfHzx"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'이상한 나라의 앨리스' 소설 데이터를 로드하고 특수문자를 제거하고 단어를 소문자화하는 간단한 전처리를 수행"
      ],
      "metadata": {
        "id": "ivbl13L7fw2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
        "f = open('11-0.txt', 'rb')\n",
        "sentences = []\n",
        "for sentence in f: # 데이터를 한 줄씩 읽는다.\n",
        "    sentence = sentence.strip() # strip()을 통해 \\r, \\n을 제거한다.\n",
        "    sentence = sentence.lower() # 소문자화.\n",
        "    sentence = sentence.decode('ascii', 'ignore') # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "mIpDq_Z5fIao"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리된 결과인 리스트에서 5개의 원소만 출력하면 이들은 문자열로 구성되어 있으므로 하나의 문자열로 통합하도록 함"
      ],
      "metadata": {
        "id": "i0ENaqqpfy66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyDLWqF8fI35",
        "outputId": "7e6afa3f-e36f-4013-edb9-dd42125e72e4"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\n",
              " 'this ebook is for the use of anyone anywhere in the united states and',\n",
              " 'most other parts of the world at no cost and with almost no restrictions',\n",
              " 'whatsoever. you may copy it, give it away or re-use it under the terms',\n",
              " 'of the project gutenberg license included with this ebook or online at']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "하나의 문자열로 통합된 길이는 15만 9천자"
      ],
      "metadata": {
        "id": "uvlivlWBf29b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_data = ' '.join(sentences)\n",
        "print('문자열의 길이 또는 총 글자의 개수: %d' % len(total_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahpenqErfJeS",
        "outputId": "ce2d849d-d4e0-4de7-ec76-68279af5d4c2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자열의 길이 또는 총 글자의 개수: 159484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_data[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIqG0YUzfKUz",
        "outputId": "4a3efdae-a001-4aae-d887-48624a1b2701"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the project gutenberg ebook of alices adventures in wonderland, by lewis carroll this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자열로부터 단어 집합이 아닌, 문자 집합을 만들도록 함"
      ],
      "metadata": {
        "id": "vUQnBWTsf6x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_vocab = sorted(list(set(total_data)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('글자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWO0rReQfK97",
        "outputId": "7ec7f3a5-aea3-4341-a458-e7da9e851337"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "글자 집합의 크기 : 56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정수, 특수문자, 구두점, 알파벳을 포함하는 문자 집합의 각 문자에 정수를 부여"
      ],
      "metadata": {
        "id": "2tTsvSiVf9it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((char, index) for index, char in enumerate(char_vocab)) # 글자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAqvXFWOfMOL",
        "outputId": "a60942d7-7ec6-4e32-834f-70e6dba60004"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, '_': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정수로부터 문자를 리턴하는 함수 생성"
      ],
      "metadata": {
        "id": "yhQSHWpdf_8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key"
      ],
      "metadata": {
        "id": "yntSO4i-fOkR"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15만 9천의 길이의 문자열로부터 샘플을 만들기 위해 문장 길이를 60으로 정하고 60으로 나누어 2658개의 샘플을 만듦"
      ],
      "metadata": {
        "id": "zS1EAW-ygC6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 60 # 문장의 길이를 60으로 한다.\n",
        "n_samples = int(np.floor((len(total_data) - 1) / seq_length)) # 문자열을 60등분한다. 그러면 즉, 총 샘플의 개수\n",
        "print ('문장 샘플의 수 : {}'.format(n_samples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F0ocIPSfPhI",
        "outputId": "27419057-782b-4873-ea7e-ed25380c9fb0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장 샘플의 수 : 2658\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리를 위해 문장을 정수 인코딩 후 오른쪽으로 한 칸 쉬프트된 문장으로 바꾸어 주도록 함"
      ],
      "metadata": {
        "id": "Ya3OKtzGgc31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = []\n",
        "train_y = []\n",
        "\n",
        "for i in range(n_samples):\n",
        "    # 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 픽한다.\n",
        "    X_sample = total_data[i * seq_length: (i + 1) * seq_length]\n",
        "\n",
        "    # 정수 인코딩\n",
        "    X_encoded = [char_to_index[c] for c in X_sample]\n",
        "    train_X.append(X_encoded)\n",
        "\n",
        "    # 오른쪽으로 1칸 쉬프트\n",
        "    y_sample = total_data[i * seq_length + 1: (i + 1) * seq_length + 1]\n",
        "    y_encoded = [char_to_index[c] for c in y_sample]\n",
        "    train_y.append(y_encoded)"
      ],
      "metadata": {
        "id": "TacBoLNifQOZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('X 데이터의 첫번째 샘플 :',train_X[0])\n",
        "print('y 데이터의 첫번째 샘플 :',train_y[0])\n",
        "print('-'*50)\n",
        "print('X 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_X[0]])\n",
        "print('y 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_y[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVS4V4l0fRBA",
        "outputId": "7980abb7-4043-402d-fe66-f8c87608e986"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X 데이터의 첫번째 샘플 : [49, 37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30]\n",
            "y 데이터의 첫번째 샘플 : [37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30, 43]\n",
            "--------------------------------------------------\n",
            "X 데이터의 첫번째 샘플 디코딩 : ['t', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a']\n",
            "y 데이터의 첫번째 샘플 디코딩 : ['h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta5bfauyfRuY",
        "outputId": "4aff6239-f53e-4a4b-a943-ed57bd1f8763"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[43, 33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_y[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eowsLTZPfSZJ",
        "outputId": "c9af53fb-c911-4855-c69c-8fefd66c0b4b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54, 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문자 단위 RNN에서는 입력 시퀀스에 대해서 워드 임베딩을 하지 않으므로 전체 데이터 뿐만 아니라 입력 시퀀스에 대해서도 원-핫 인코딩을 수행"
      ],
      "metadata": {
        "id": "yX59SxyYgkFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = to_categorical(train_X)\n",
        "train_y = to_categorical(train_y)"
      ],
      "metadata": {
        "id": "kEwwpLAIfS-K"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('train_X의 크기(shape) : {}'.format(train_X.shape)) # 원-핫 인코딩\n",
        "print('train_y의 크기(shape) : {}'.format(train_y.shape)) # 원-핫 인코딩"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpY6LLPafUcj",
        "outputId": "ba501952-dcb0-418b-b0eb-6ddc4f9420ac"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X의 크기(shape) : (2658, 60, 56)\n",
            "train_y의 크기(shape) : (2658, 60, 56)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) 모델 설계하기**"
      ],
      "metadata": {
        "id": "__QxTlIQe_Wy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터인 은닉 상태의 크기는 256로 하고 다 대 다 구조의 LSTM을 사용하며 LSTM 은닉층은 두 개를 사용함<br>\n",
        "그리고 전결합층을 출력층으로 하여 문자 집합 크기만큼의 뉴런을 배치하여 모델을 설계하도록 함<br>\n",
        "해당 모델은 모든 시점에서 모든 가능한 문자 중 하나의 문자를 예측하는 다중 클래스 분류 문제를 수행하는 것이므로<br>\n",
        "출력층에서 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로는 크로스 엔트로피 함수를 사용하여 80 에포크 수행"
      ],
      "metadata": {
        "id": "uzQlkzi1g000"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
        "\n",
        "hidden_units = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(hidden_units, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uypbo7aufVI1",
        "outputId": "11195d87-54e6-4e38-a094-9568e54a6fae"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, None, 256)         320512    \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, None, 256)         525312    \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, None, 56)         14392     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 860,216\n",
            "Trainable params: 860,216\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_X, train_y, epochs=80, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpZxXybPfV1q",
        "outputId": "160e50c6-0c06-4a96-a124-46f11dbd1b2a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "84/84 - 4s - loss: 3.0669 - accuracy: 0.1830 - 4s/epoch - 42ms/step\n",
            "Epoch 2/80\n",
            "84/84 - 1s - loss: 2.7372 - accuracy: 0.2457 - 799ms/epoch - 10ms/step\n",
            "Epoch 3/80\n",
            "84/84 - 1s - loss: 2.3997 - accuracy: 0.3264 - 838ms/epoch - 10ms/step\n",
            "Epoch 4/80\n",
            "84/84 - 1s - loss: 2.2594 - accuracy: 0.3560 - 811ms/epoch - 10ms/step\n",
            "Epoch 5/80\n",
            "84/84 - 1s - loss: 2.1568 - accuracy: 0.3840 - 834ms/epoch - 10ms/step\n",
            "Epoch 6/80\n",
            "84/84 - 1s - loss: 2.0705 - accuracy: 0.4050 - 833ms/epoch - 10ms/step\n",
            "Epoch 7/80\n",
            "84/84 - 1s - loss: 2.0000 - accuracy: 0.4236 - 794ms/epoch - 9ms/step\n",
            "Epoch 8/80\n",
            "84/84 - 1s - loss: 1.9450 - accuracy: 0.4382 - 807ms/epoch - 10ms/step\n",
            "Epoch 9/80\n",
            "84/84 - 1s - loss: 1.8941 - accuracy: 0.4515 - 815ms/epoch - 10ms/step\n",
            "Epoch 10/80\n",
            "84/84 - 1s - loss: 1.8482 - accuracy: 0.4638 - 793ms/epoch - 9ms/step\n",
            "Epoch 11/80\n",
            "84/84 - 1s - loss: 1.8076 - accuracy: 0.4741 - 819ms/epoch - 10ms/step\n",
            "Epoch 12/80\n",
            "84/84 - 1s - loss: 1.7707 - accuracy: 0.4853 - 824ms/epoch - 10ms/step\n",
            "Epoch 13/80\n",
            "84/84 - 1s - loss: 1.7344 - accuracy: 0.4945 - 806ms/epoch - 10ms/step\n",
            "Epoch 14/80\n",
            "84/84 - 1s - loss: 1.7019 - accuracy: 0.5037 - 879ms/epoch - 10ms/step\n",
            "Epoch 15/80\n",
            "84/84 - 1s - loss: 1.6693 - accuracy: 0.5123 - 851ms/epoch - 10ms/step\n",
            "Epoch 16/80\n",
            "84/84 - 1s - loss: 1.6388 - accuracy: 0.5203 - 979ms/epoch - 12ms/step\n",
            "Epoch 17/80\n",
            "84/84 - 1s - loss: 1.6099 - accuracy: 0.5275 - 1s/epoch - 13ms/step\n",
            "Epoch 18/80\n",
            "84/84 - 1s - loss: 1.5812 - accuracy: 0.5352 - 827ms/epoch - 10ms/step\n",
            "Epoch 19/80\n",
            "84/84 - 1s - loss: 1.5523 - accuracy: 0.5423 - 834ms/epoch - 10ms/step\n",
            "Epoch 20/80\n",
            "84/84 - 1s - loss: 1.5260 - accuracy: 0.5495 - 812ms/epoch - 10ms/step\n",
            "Epoch 21/80\n",
            "84/84 - 1s - loss: 1.4984 - accuracy: 0.5573 - 842ms/epoch - 10ms/step\n",
            "Epoch 22/80\n",
            "84/84 - 1s - loss: 1.4727 - accuracy: 0.5637 - 833ms/epoch - 10ms/step\n",
            "Epoch 23/80\n",
            "84/84 - 1s - loss: 1.4496 - accuracy: 0.5706 - 868ms/epoch - 10ms/step\n",
            "Epoch 24/80\n",
            "84/84 - 1s - loss: 1.4212 - accuracy: 0.5785 - 841ms/epoch - 10ms/step\n",
            "Epoch 25/80\n",
            "84/84 - 1s - loss: 1.3961 - accuracy: 0.5863 - 823ms/epoch - 10ms/step\n",
            "Epoch 26/80\n",
            "84/84 - 1s - loss: 1.3729 - accuracy: 0.5933 - 837ms/epoch - 10ms/step\n",
            "Epoch 27/80\n",
            "84/84 - 1s - loss: 1.3452 - accuracy: 0.6008 - 832ms/epoch - 10ms/step\n",
            "Epoch 28/80\n",
            "84/84 - 1s - loss: 1.3227 - accuracy: 0.6069 - 816ms/epoch - 10ms/step\n",
            "Epoch 29/80\n",
            "84/84 - 1s - loss: 1.2990 - accuracy: 0.6134 - 859ms/epoch - 10ms/step\n",
            "Epoch 30/80\n",
            "84/84 - 1s - loss: 1.2724 - accuracy: 0.6221 - 843ms/epoch - 10ms/step\n",
            "Epoch 31/80\n",
            "84/84 - 1s - loss: 1.2461 - accuracy: 0.6296 - 810ms/epoch - 10ms/step\n",
            "Epoch 32/80\n",
            "84/84 - 1s - loss: 1.2239 - accuracy: 0.6352 - 823ms/epoch - 10ms/step\n",
            "Epoch 33/80\n",
            "84/84 - 1s - loss: 1.2003 - accuracy: 0.6412 - 829ms/epoch - 10ms/step\n",
            "Epoch 34/80\n",
            "84/84 - 1s - loss: 1.1724 - accuracy: 0.6490 - 819ms/epoch - 10ms/step\n",
            "Epoch 35/80\n",
            "84/84 - 1s - loss: 1.1517 - accuracy: 0.6547 - 822ms/epoch - 10ms/step\n",
            "Epoch 36/80\n",
            "84/84 - 1s - loss: 1.1238 - accuracy: 0.6644 - 839ms/epoch - 10ms/step\n",
            "Epoch 37/80\n",
            "84/84 - 1s - loss: 1.0998 - accuracy: 0.6712 - 801ms/epoch - 10ms/step\n",
            "Epoch 38/80\n",
            "84/84 - 1s - loss: 1.0785 - accuracy: 0.6771 - 829ms/epoch - 10ms/step\n",
            "Epoch 39/80\n",
            "84/84 - 1s - loss: 1.0521 - accuracy: 0.6851 - 824ms/epoch - 10ms/step\n",
            "Epoch 40/80\n",
            "84/84 - 1s - loss: 1.0247 - accuracy: 0.6931 - 831ms/epoch - 10ms/step\n",
            "Epoch 41/80\n",
            "84/84 - 1s - loss: 1.0054 - accuracy: 0.6984 - 820ms/epoch - 10ms/step\n",
            "Epoch 42/80\n",
            "84/84 - 1s - loss: 0.9737 - accuracy: 0.7091 - 819ms/epoch - 10ms/step\n",
            "Epoch 43/80\n",
            "84/84 - 1s - loss: 0.9474 - accuracy: 0.7166 - 815ms/epoch - 10ms/step\n",
            "Epoch 44/80\n",
            "84/84 - 1s - loss: 0.9230 - accuracy: 0.7247 - 819ms/epoch - 10ms/step\n",
            "Epoch 45/80\n",
            "84/84 - 1s - loss: 0.9015 - accuracy: 0.7315 - 834ms/epoch - 10ms/step\n",
            "Epoch 46/80\n",
            "84/84 - 1s - loss: 0.8741 - accuracy: 0.7388 - 815ms/epoch - 10ms/step\n",
            "Epoch 47/80\n",
            "84/84 - 1s - loss: 0.8455 - accuracy: 0.7493 - 817ms/epoch - 10ms/step\n",
            "Epoch 48/80\n",
            "84/84 - 1s - loss: 0.8281 - accuracy: 0.7543 - 819ms/epoch - 10ms/step\n",
            "Epoch 49/80\n",
            "84/84 - 1s - loss: 0.8058 - accuracy: 0.7613 - 839ms/epoch - 10ms/step\n",
            "Epoch 50/80\n",
            "84/84 - 1s - loss: 0.7753 - accuracy: 0.7713 - 839ms/epoch - 10ms/step\n",
            "Epoch 51/80\n",
            "84/84 - 1s - loss: 0.7554 - accuracy: 0.7774 - 836ms/epoch - 10ms/step\n",
            "Epoch 52/80\n",
            "84/84 - 1s - loss: 0.7336 - accuracy: 0.7844 - 819ms/epoch - 10ms/step\n",
            "Epoch 53/80\n",
            "84/84 - 1s - loss: 0.7104 - accuracy: 0.7915 - 801ms/epoch - 10ms/step\n",
            "Epoch 54/80\n",
            "84/84 - 1s - loss: 0.6977 - accuracy: 0.7952 - 811ms/epoch - 10ms/step\n",
            "Epoch 55/80\n",
            "84/84 - 1s - loss: 0.6612 - accuracy: 0.8085 - 809ms/epoch - 10ms/step\n",
            "Epoch 56/80\n",
            "84/84 - 1s - loss: 0.6442 - accuracy: 0.8143 - 813ms/epoch - 10ms/step\n",
            "Epoch 57/80\n",
            "84/84 - 1s - loss: 0.6242 - accuracy: 0.8198 - 845ms/epoch - 10ms/step\n",
            "Epoch 58/80\n",
            "84/84 - 1s - loss: 0.6025 - accuracy: 0.8265 - 800ms/epoch - 10ms/step\n",
            "Epoch 59/80\n",
            "84/84 - 1s - loss: 0.5874 - accuracy: 0.8299 - 825ms/epoch - 10ms/step\n",
            "Epoch 60/80\n",
            "84/84 - 1s - loss: 0.5596 - accuracy: 0.8401 - 814ms/epoch - 10ms/step\n",
            "Epoch 61/80\n",
            "84/84 - 1s - loss: 0.5540 - accuracy: 0.8407 - 820ms/epoch - 10ms/step\n",
            "Epoch 62/80\n",
            "84/84 - 1s - loss: 0.5220 - accuracy: 0.8526 - 809ms/epoch - 10ms/step\n",
            "Epoch 63/80\n",
            "84/84 - 1s - loss: 0.5044 - accuracy: 0.8589 - 810ms/epoch - 10ms/step\n",
            "Epoch 64/80\n",
            "84/84 - 1s - loss: 0.4914 - accuracy: 0.8620 - 834ms/epoch - 10ms/step\n",
            "Epoch 65/80\n",
            "84/84 - 1s - loss: 0.4779 - accuracy: 0.8665 - 830ms/epoch - 10ms/step\n",
            "Epoch 66/80\n",
            "84/84 - 1s - loss: 0.4571 - accuracy: 0.8732 - 825ms/epoch - 10ms/step\n",
            "Epoch 67/80\n",
            "84/84 - 1s - loss: 0.4341 - accuracy: 0.8816 - 833ms/epoch - 10ms/step\n",
            "Epoch 68/80\n",
            "84/84 - 1s - loss: 0.4279 - accuracy: 0.8822 - 793ms/epoch - 9ms/step\n",
            "Epoch 69/80\n",
            "84/84 - 1s - loss: 0.4100 - accuracy: 0.8885 - 814ms/epoch - 10ms/step\n",
            "Epoch 70/80\n",
            "84/84 - 1s - loss: 0.3979 - accuracy: 0.8920 - 818ms/epoch - 10ms/step\n",
            "Epoch 71/80\n",
            "84/84 - 1s - loss: 0.3959 - accuracy: 0.8904 - 805ms/epoch - 10ms/step\n",
            "Epoch 72/80\n",
            "84/84 - 1s - loss: 0.3798 - accuracy: 0.8972 - 820ms/epoch - 10ms/step\n",
            "Epoch 73/80\n",
            "84/84 - 1s - loss: 0.3571 - accuracy: 0.9047 - 809ms/epoch - 10ms/step\n",
            "Epoch 74/80\n",
            "84/84 - 1s - loss: 0.3551 - accuracy: 0.9043 - 833ms/epoch - 10ms/step\n",
            "Epoch 75/80\n",
            "84/84 - 1s - loss: 0.3711 - accuracy: 0.8952 - 812ms/epoch - 10ms/step\n",
            "Epoch 76/80\n",
            "84/84 - 1s - loss: 0.3310 - accuracy: 0.9126 - 809ms/epoch - 10ms/step\n",
            "Epoch 77/80\n",
            "84/84 - 1s - loss: 0.3064 - accuracy: 0.9213 - 851ms/epoch - 10ms/step\n",
            "Epoch 78/80\n",
            "84/84 - 1s - loss: 0.2891 - accuracy: 0.9274 - 813ms/epoch - 10ms/step\n",
            "Epoch 79/80\n",
            "84/84 - 1s - loss: 0.2765 - accuracy: 0.9314 - 824ms/epoch - 10ms/step\n",
            "Epoch 80/80\n",
            "84/84 - 1s - loss: 0.2628 - accuracy: 0.9357 - 811ms/epoch - 10ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbe17345390>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "특정 문자를 주면 다음 문자를 계속해서 생성해내는 함수를 구현하고 인자로 학습 모델, 문자를 몇 번 생성할 것인지 횟수를 전달하면<br> \n",
        "해당 함수는 임의로 시작 문자를 정한 뒤 정해진 횟수만큼 다음 문자를 지속적으로 예측하여 문장을 생성함"
      ],
      "metadata": {
        "id": "5M0AC-lyg7bM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, length):\n",
        "    # 글자에 대한 랜덤 인덱스 생성\n",
        "    ix = [np.random.randint(vocab_size)]\n",
        "\n",
        "    # 랜덤 익덱스로부터 글자 생성\n",
        "    y_char = [index_to_char[ix[-1]]]\n",
        "    print(ix[-1],'번 글자',y_char[-1],'로 예측을 시작!')\n",
        "\n",
        "    # (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\n",
        "    X = np.zeros((1, length, vocab_size))\n",
        "\n",
        "    for i in range(length):\n",
        "        # X[0][i][예측한 글자의 인덱스] = 1, 즉, 예측 글자를 다음 입력 시퀀스에 추가\n",
        "        X[0][i][ix[-1]] = 1\n",
        "        print(index_to_char[ix[-1]], end=\"\")\n",
        "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
        "        y_char.append(index_to_char[ix[-1]])\n",
        "    return ('').join(y_char)"
      ],
      "metadata": {
        "id": "6TrElJ7ffW5T"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = sentence_generation(model, 100)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeKhWKoPfXkS",
        "outputId": "dc20856c-852e-4885-851f-cb1338e96ad0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38 번 글자 i 로 예측을 시작!\n",
            "1/1 [==============================] - 1s 634ms/step\n",
            "1/1 [==============================] - 1s 609ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 13ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 14ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 15ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "ing voice. theres more evidence to come yet, please your maje_ty, said the king, looking round one, a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문자 단위 RNN으로 텍스트 생성하기"
      ],
      "metadata": {
        "id": "Y2Tq5wYXfAlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문자 단위 RNN 언어 모델 구현하기 (다 대 일 LSTM)"
      ],
      "metadata": {
        "id": "Le8mpooZfDK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) 데이터에 대한 이해와 전처리**"
      ],
      "metadata": {
        "id": "QN-rp3AVfEDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "LD6iH0JFfZST"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "엉터리 노래 가사를 로드"
      ],
      "metadata": {
        "id": "pVjUma4BhScJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = '''\n",
        "I get on with life as a programmer,\n",
        "I like to contemplate beer.\n",
        "But when I start to daydream,\n",
        "My mind turns straight to wine.\n",
        "\n",
        "Do I love wine more than beer?\n",
        "\n",
        "I like to use words about beer.\n",
        "But when I stop my talking,\n",
        "My mind turns straight to wine.\n",
        "\n",
        "I hate bugs and errors.\n",
        "But I just think back to wine,\n",
        "And I'm happy once again.\n",
        "\n",
        "I like to hang out with programming and deep learning.\n",
        "But when left alone,\n",
        "My mind turns straight to wine.\n",
        "'''"
      ],
      "metadata": {
        "id": "aWaq8MKQfaIx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트에 존재하는 단락 구분을 없애고 하나의 문자열로 재저장 후 문자 집합을 생성"
      ],
      "metadata": {
        "id": "MtLhgb6XhUWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "print(raw_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzvHJgOFfbeK",
        "outputId": "d3a58652-bbfe-4b11-8e50-cc3fe081d852"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I get on with life as a programmer, I like to contemplate beer. But when I start to daydream, My mind turns straight to wine. Do I love wine more than beer? I like to use words about beer. But when I stop my talking, My mind turns straight to wine. I hate bugs and errors. But I just think back to wine, And I'm happy once again. I like to hang out with programming and deep learning. But when left alone, My mind turns straight to wine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복을 제거한 글자 집합 생성\n",
        "char_vocab = sorted(list(set(raw_text)))\n",
        "print(char_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbS3PEkRfcQr",
        "outputId": "22d122a5-5e80-4a58-d242-e1f859813ae2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', \"'\", ',', '.', '?', 'A', 'B', 'D', 'I', 'M', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(char_vocab)\n",
        "print ('글자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0DROXAifdGZ",
        "outputId": "8d14feef-001b-49bf-8d5a-46d9279bbb87"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "글자 집합의 크기 : 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "대, 소문자 알파벳 또는 구두점 등의 단위의 집합인 문자 집합이 만들어지면 각 문자에 정수를 부여하도록 함"
      ],
      "metadata": {
        "id": "Rj0qYEljhXvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 글자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca-DUGNKgTGS",
        "outputId": "76e478ad-afb4-4d72-9557-1a44a4580c3d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 0, \"'\": 1, ',': 2, '.': 3, '?': 4, 'A': 5, 'B': 6, 'D': 7, 'I': 8, 'M': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'y': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 시퀀스의 길이가 10이 되도록 데이터를 구성하며 예측 대상인 문자도 필요하므로 길이가 11이 되도록 구성"
      ],
      "metadata": {
        "id": "PWTWMcvjhaJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = 11\n",
        "sequences = []\n",
        "for i in range(length, len(raw_text)):\n",
        "    seq = raw_text[i-length:i] # 길이 11의 문자열을 지속적으로 만든다.\n",
        "    sequences.append(seq)\n",
        "print('총 훈련 샘플의 수: %d' % len(sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxruhHSzfeQy",
        "outputId": "0c3a23df-0a56-4b42-8bd1-83a9d793ab80"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 훈련 샘플의 수: 426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "총 샘플의 수는 426개가 되므로 10개를 출력하면 엉터리 노래의 첫 문장이 10개의 샘플로 분리된 것을 확인할 수 있음"
      ],
      "metadata": {
        "id": "jw8JfJlZhcJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naqBWREbffFM",
        "outputId": "f16bc6de-6888-4b04-edc3-80872f6f463e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I get on wi',\n",
              " ' get on wit',\n",
              " 'get on with',\n",
              " 'et on with ',\n",
              " 't on with l',\n",
              " ' on with li',\n",
              " 'on with lif',\n",
              " 'n with life',\n",
              " ' with life ',\n",
              " 'with life a']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전처리를 위해 문자 집합에 정수 인덱스를 부여한 것을 가지고 전체 데이터에 대해 정수 인코딩을 수행"
      ],
      "metadata": {
        "id": "fX2QN3yhhzsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = []\n",
        "for sequence in sequences: # 전체 데이터에서 문장 샘플을 1개씩 꺼낸다.\n",
        "    encoded_sequence = [char_to_index[char] for char in sequence] # 문장 샘플에서 각 글자에 대해서 정수 인코딩을 수행.\n",
        "    encoded_sequences.append(encoded_sequence)"
      ],
      "metadata": {
        "id": "RheVmyNHfgYC"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPU78d2jfg_Z",
        "outputId": "f7528472-bb8f-4437-adcf-33a210a2b1cc"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[8, 0, 16, 14, 28, 0, 24, 23, 0, 31, 18],\n",
              " [0, 16, 14, 28, 0, 24, 23, 0, 31, 18, 28],\n",
              " [16, 14, 28, 0, 24, 23, 0, 31, 18, 28, 17],\n",
              " [14, 28, 0, 24, 23, 0, 31, 18, 28, 17, 0],\n",
              " [28, 0, 24, 23, 0, 31, 18, 28, 17, 0, 21]]"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "예측 대상인 문자를 분리시켜주기 위해 모든 샘플 문장에 대해서 마지막 문자를 분리"
      ],
      "metadata": {
        "id": "zYCAStHZh9Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_sequences = np.array(encoded_sequences)\n",
        "X_data = encoded_sequences[:,:-1]\n",
        "\n",
        "# 맨 마지막 위치의 글자를 분리\n",
        "y_data = encoded_sequences[:,-1]"
      ],
      "metadata": {
        "id": "IasKAqmSfiIR"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data[:5])\n",
        "print(y_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGNzz76vfjAj",
        "outputId": "f1f7d5f0-2f74-4166-d42a-2778c560c58b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8  0 16 14 28  0 24 23  0 31]\n",
            " [ 0 16 14 28  0 24 23  0 31 18]\n",
            " [16 14 28  0 24 23  0 31 18 28]\n",
            " [14 28  0 24 23  0 31 18 28 17]\n",
            " [28  0 24 23  0 31 18 28 17  0]]\n",
            "[18 28 17  0 21]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "정상적으로 분리가 되었다면 분리된 샘플과 마지막 문자에 대해서 원-핫 인코딩을 수행"
      ],
      "metadata": {
        "id": "rK0CdhZQh_RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 원-핫 인코딩\n",
        "X_data_one_hot = [to_categorical(encoded, num_classes=vocab_size) for encoded in X_data]\n",
        "X_data_one_hot = np.array(X_data_one_hot)\n",
        "y_data_one_hot = to_categorical(y_data, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "wPNQQdJqfjtJ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_data_one_hot.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekALlczDfkfx",
        "outputId": "e7cb0576-ec27-42d8-86fa-90cdd44308c2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(426, 10, 33)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) 모델 설계하기**"
      ],
      "metadata": {
        "id": "w5dMWCQmfFXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "CJcB79O0flEL"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터인 은닉 상태의 크기는 64로 하고 다 대 일 구조의 LSTM을 사용<br>\n",
        "그리고 전결합층을 출력층으로 하여 문자 집합 크기만큼의 뉴런을 배치하여 모델을 설계하도록 함<br>\n",
        "해당 모델은 마지막 시점에서 모든 가능한 문자 중 하나의 문자를 예측하는 다중 클래스 분류 문제를 수행하는 것이므로<br>\n",
        "출력층에서 활성화 함수로는 소프트맥스 함수를 사용하고, 손실 함수로는 크로스 엔트로피 함수를 사용하여 100 에포크 수행"
      ],
      "metadata": {
        "id": "EKDlN_eOiEPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_units = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_units, input_shape=(X_data_one_hot.shape[1], X_data_one_hot.shape[2])))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ],
      "metadata": {
        "id": "B6PHvXpQfmTZ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_data_one_hot, y_data_one_hot, epochs=100, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3Eukg09fm-B",
        "outputId": "3a648f27-fce6-4e02-c49f-b2a48d713237"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "14/14 - 1s - loss: 3.4742 - accuracy: 0.1103 - 1s/epoch - 105ms/step\n",
            "Epoch 2/100\n",
            "14/14 - 0s - loss: 3.3409 - accuracy: 0.1948 - 66ms/epoch - 5ms/step\n",
            "Epoch 3/100\n",
            "14/14 - 0s - loss: 3.0741 - accuracy: 0.1972 - 49ms/epoch - 4ms/step\n",
            "Epoch 4/100\n",
            "14/14 - 0s - loss: 3.0021 - accuracy: 0.1972 - 52ms/epoch - 4ms/step\n",
            "Epoch 5/100\n",
            "14/14 - 0s - loss: 2.9626 - accuracy: 0.1972 - 50ms/epoch - 4ms/step\n",
            "Epoch 6/100\n",
            "14/14 - 0s - loss: 2.9545 - accuracy: 0.1972 - 56ms/epoch - 4ms/step\n",
            "Epoch 7/100\n",
            "14/14 - 0s - loss: 2.9436 - accuracy: 0.1972 - 52ms/epoch - 4ms/step\n",
            "Epoch 8/100\n",
            "14/14 - 0s - loss: 2.9256 - accuracy: 0.1972 - 49ms/epoch - 4ms/step\n",
            "Epoch 9/100\n",
            "14/14 - 0s - loss: 2.9147 - accuracy: 0.1972 - 50ms/epoch - 4ms/step\n",
            "Epoch 10/100\n",
            "14/14 - 0s - loss: 2.8979 - accuracy: 0.1972 - 52ms/epoch - 4ms/step\n",
            "Epoch 11/100\n",
            "14/14 - 0s - loss: 2.8862 - accuracy: 0.1972 - 49ms/epoch - 4ms/step\n",
            "Epoch 12/100\n",
            "14/14 - 0s - loss: 2.8614 - accuracy: 0.1972 - 58ms/epoch - 4ms/step\n",
            "Epoch 13/100\n",
            "14/14 - 0s - loss: 2.8308 - accuracy: 0.1972 - 50ms/epoch - 4ms/step\n",
            "Epoch 14/100\n",
            "14/14 - 0s - loss: 2.8060 - accuracy: 0.2019 - 51ms/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "14/14 - 0s - loss: 2.7773 - accuracy: 0.2113 - 49ms/epoch - 4ms/step\n",
            "Epoch 16/100\n",
            "14/14 - 0s - loss: 2.7449 - accuracy: 0.2324 - 50ms/epoch - 4ms/step\n",
            "Epoch 17/100\n",
            "14/14 - 0s - loss: 2.6958 - accuracy: 0.2277 - 50ms/epoch - 4ms/step\n",
            "Epoch 18/100\n",
            "14/14 - 0s - loss: 2.6732 - accuracy: 0.2371 - 53ms/epoch - 4ms/step\n",
            "Epoch 19/100\n",
            "14/14 - 0s - loss: 2.6267 - accuracy: 0.2394 - 52ms/epoch - 4ms/step\n",
            "Epoch 20/100\n",
            "14/14 - 0s - loss: 2.5904 - accuracy: 0.2582 - 52ms/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "14/14 - 0s - loss: 2.5509 - accuracy: 0.2770 - 56ms/epoch - 4ms/step\n",
            "Epoch 22/100\n",
            "14/14 - 0s - loss: 2.5147 - accuracy: 0.2840 - 49ms/epoch - 3ms/step\n",
            "Epoch 23/100\n",
            "14/14 - 0s - loss: 2.5009 - accuracy: 0.3075 - 50ms/epoch - 4ms/step\n",
            "Epoch 24/100\n",
            "14/14 - 0s - loss: 2.4355 - accuracy: 0.3122 - 54ms/epoch - 4ms/step\n",
            "Epoch 25/100\n",
            "14/14 - 0s - loss: 2.4054 - accuracy: 0.3286 - 49ms/epoch - 4ms/step\n",
            "Epoch 26/100\n",
            "14/14 - 0s - loss: 2.3666 - accuracy: 0.3333 - 50ms/epoch - 4ms/step\n",
            "Epoch 27/100\n",
            "14/14 - 0s - loss: 2.3329 - accuracy: 0.3521 - 52ms/epoch - 4ms/step\n",
            "Epoch 28/100\n",
            "14/14 - 0s - loss: 2.3015 - accuracy: 0.3474 - 48ms/epoch - 3ms/step\n",
            "Epoch 29/100\n",
            "14/14 - 0s - loss: 2.2792 - accuracy: 0.3451 - 54ms/epoch - 4ms/step\n",
            "Epoch 30/100\n",
            "14/14 - 0s - loss: 2.2265 - accuracy: 0.3826 - 50ms/epoch - 4ms/step\n",
            "Epoch 31/100\n",
            "14/14 - 0s - loss: 2.1961 - accuracy: 0.3685 - 49ms/epoch - 3ms/step\n",
            "Epoch 32/100\n",
            "14/14 - 0s - loss: 2.1450 - accuracy: 0.3991 - 52ms/epoch - 4ms/step\n",
            "Epoch 33/100\n",
            "14/14 - 0s - loss: 2.1166 - accuracy: 0.4085 - 59ms/epoch - 4ms/step\n",
            "Epoch 34/100\n",
            "14/14 - 0s - loss: 2.0692 - accuracy: 0.4178 - 49ms/epoch - 4ms/step\n",
            "Epoch 35/100\n",
            "14/14 - 0s - loss: 2.0783 - accuracy: 0.3944 - 48ms/epoch - 3ms/step\n",
            "Epoch 36/100\n",
            "14/14 - 0s - loss: 1.9886 - accuracy: 0.4460 - 59ms/epoch - 4ms/step\n",
            "Epoch 37/100\n",
            "14/14 - 0s - loss: 1.9566 - accuracy: 0.4460 - 49ms/epoch - 3ms/step\n",
            "Epoch 38/100\n",
            "14/14 - 0s - loss: 1.9233 - accuracy: 0.4577 - 52ms/epoch - 4ms/step\n",
            "Epoch 39/100\n",
            "14/14 - 0s - loss: 1.9055 - accuracy: 0.4859 - 47ms/epoch - 3ms/step\n",
            "Epoch 40/100\n",
            "14/14 - 0s - loss: 1.8581 - accuracy: 0.4437 - 47ms/epoch - 3ms/step\n",
            "Epoch 41/100\n",
            "14/14 - 0s - loss: 1.8078 - accuracy: 0.4930 - 48ms/epoch - 3ms/step\n",
            "Epoch 42/100\n",
            "14/14 - 0s - loss: 1.7773 - accuracy: 0.5023 - 50ms/epoch - 4ms/step\n",
            "Epoch 43/100\n",
            "14/14 - 0s - loss: 1.7249 - accuracy: 0.5516 - 48ms/epoch - 3ms/step\n",
            "Epoch 44/100\n",
            "14/14 - 0s - loss: 1.6901 - accuracy: 0.5681 - 56ms/epoch - 4ms/step\n",
            "Epoch 45/100\n",
            "14/14 - 0s - loss: 1.6797 - accuracy: 0.5634 - 53ms/epoch - 4ms/step\n",
            "Epoch 46/100\n",
            "14/14 - 0s - loss: 1.6386 - accuracy: 0.5822 - 50ms/epoch - 4ms/step\n",
            "Epoch 47/100\n",
            "14/14 - 0s - loss: 1.5889 - accuracy: 0.6009 - 49ms/epoch - 3ms/step\n",
            "Epoch 48/100\n",
            "14/14 - 0s - loss: 1.5417 - accuracy: 0.6127 - 50ms/epoch - 4ms/step\n",
            "Epoch 49/100\n",
            "14/14 - 0s - loss: 1.5184 - accuracy: 0.6174 - 47ms/epoch - 3ms/step\n",
            "Epoch 50/100\n",
            "14/14 - 0s - loss: 1.4860 - accuracy: 0.6291 - 46ms/epoch - 3ms/step\n",
            "Epoch 51/100\n",
            "14/14 - 0s - loss: 1.4813 - accuracy: 0.6315 - 47ms/epoch - 3ms/step\n",
            "Epoch 52/100\n",
            "14/14 - 0s - loss: 1.4133 - accuracy: 0.6620 - 48ms/epoch - 3ms/step\n",
            "Epoch 53/100\n",
            "14/14 - 0s - loss: 1.3724 - accuracy: 0.6808 - 49ms/epoch - 4ms/step\n",
            "Epoch 54/100\n",
            "14/14 - 0s - loss: 1.3462 - accuracy: 0.6667 - 48ms/epoch - 3ms/step\n",
            "Epoch 55/100\n",
            "14/14 - 0s - loss: 1.3124 - accuracy: 0.6808 - 58ms/epoch - 4ms/step\n",
            "Epoch 56/100\n",
            "14/14 - 0s - loss: 1.2697 - accuracy: 0.7113 - 48ms/epoch - 3ms/step\n",
            "Epoch 57/100\n",
            "14/14 - 0s - loss: 1.2421 - accuracy: 0.7019 - 49ms/epoch - 3ms/step\n",
            "Epoch 58/100\n",
            "14/14 - 0s - loss: 1.2250 - accuracy: 0.6972 - 50ms/epoch - 4ms/step\n",
            "Epoch 59/100\n",
            "14/14 - 0s - loss: 1.1942 - accuracy: 0.7254 - 46ms/epoch - 3ms/step\n",
            "Epoch 60/100\n",
            "14/14 - 0s - loss: 1.1470 - accuracy: 0.7488 - 47ms/epoch - 3ms/step\n",
            "Epoch 61/100\n",
            "14/14 - 0s - loss: 1.1155 - accuracy: 0.7512 - 48ms/epoch - 3ms/step\n",
            "Epoch 62/100\n",
            "14/14 - 0s - loss: 1.0913 - accuracy: 0.7676 - 52ms/epoch - 4ms/step\n",
            "Epoch 63/100\n",
            "14/14 - 0s - loss: 1.0556 - accuracy: 0.7723 - 48ms/epoch - 3ms/step\n",
            "Epoch 64/100\n",
            "14/14 - 0s - loss: 1.0319 - accuracy: 0.7840 - 50ms/epoch - 4ms/step\n",
            "Epoch 65/100\n",
            "14/14 - 0s - loss: 1.0114 - accuracy: 0.7911 - 47ms/epoch - 3ms/step\n",
            "Epoch 66/100\n",
            "14/14 - 0s - loss: 0.9979 - accuracy: 0.7793 - 58ms/epoch - 4ms/step\n",
            "Epoch 67/100\n",
            "14/14 - 0s - loss: 0.9423 - accuracy: 0.7934 - 49ms/epoch - 3ms/step\n",
            "Epoch 68/100\n",
            "14/14 - 0s - loss: 0.9112 - accuracy: 0.8192 - 47ms/epoch - 3ms/step\n",
            "Epoch 69/100\n",
            "14/14 - 0s - loss: 0.8993 - accuracy: 0.8239 - 50ms/epoch - 4ms/step\n",
            "Epoch 70/100\n",
            "14/14 - 0s - loss: 0.8651 - accuracy: 0.8263 - 49ms/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "14/14 - 0s - loss: 0.8284 - accuracy: 0.8286 - 56ms/epoch - 4ms/step\n",
            "Epoch 72/100\n",
            "14/14 - 0s - loss: 0.8064 - accuracy: 0.8638 - 49ms/epoch - 3ms/step\n",
            "Epoch 73/100\n",
            "14/14 - 0s - loss: 0.7964 - accuracy: 0.8380 - 55ms/epoch - 4ms/step\n",
            "Epoch 74/100\n",
            "14/14 - 0s - loss: 0.7629 - accuracy: 0.8498 - 49ms/epoch - 3ms/step\n",
            "Epoch 75/100\n",
            "14/14 - 0s - loss: 0.7390 - accuracy: 0.8521 - 50ms/epoch - 4ms/step\n",
            "Epoch 76/100\n",
            "14/14 - 0s - loss: 0.7160 - accuracy: 0.8662 - 49ms/epoch - 3ms/step\n",
            "Epoch 77/100\n",
            "14/14 - 0s - loss: 0.6934 - accuracy: 0.8615 - 49ms/epoch - 3ms/step\n",
            "Epoch 78/100\n",
            "14/14 - 0s - loss: 0.6658 - accuracy: 0.8803 - 48ms/epoch - 3ms/step\n",
            "Epoch 79/100\n",
            "14/14 - 0s - loss: 0.6603 - accuracy: 0.8709 - 47ms/epoch - 3ms/step\n",
            "Epoch 80/100\n",
            "14/14 - 0s - loss: 0.6359 - accuracy: 0.8944 - 53ms/epoch - 4ms/step\n",
            "Epoch 81/100\n",
            "14/14 - 0s - loss: 0.6382 - accuracy: 0.8779 - 52ms/epoch - 4ms/step\n",
            "Epoch 82/100\n",
            "14/14 - 0s - loss: 0.6126 - accuracy: 0.8873 - 49ms/epoch - 4ms/step\n",
            "Epoch 83/100\n",
            "14/14 - 0s - loss: 0.5990 - accuracy: 0.8991 - 48ms/epoch - 3ms/step\n",
            "Epoch 84/100\n",
            "14/14 - 0s - loss: 0.5701 - accuracy: 0.9038 - 49ms/epoch - 3ms/step\n",
            "Epoch 85/100\n",
            "14/14 - 0s - loss: 0.5474 - accuracy: 0.9014 - 48ms/epoch - 3ms/step\n",
            "Epoch 86/100\n",
            "14/14 - 0s - loss: 0.5197 - accuracy: 0.9178 - 48ms/epoch - 3ms/step\n",
            "Epoch 87/100\n",
            "14/14 - 0s - loss: 0.5007 - accuracy: 0.9178 - 50ms/epoch - 4ms/step\n",
            "Epoch 88/100\n",
            "14/14 - 0s - loss: 0.4877 - accuracy: 0.9272 - 48ms/epoch - 3ms/step\n",
            "Epoch 89/100\n",
            "14/14 - 0s - loss: 0.4740 - accuracy: 0.9249 - 48ms/epoch - 3ms/step\n",
            "Epoch 90/100\n",
            "14/14 - 0s - loss: 0.4631 - accuracy: 0.9296 - 49ms/epoch - 3ms/step\n",
            "Epoch 91/100\n",
            "14/14 - 0s - loss: 0.4519 - accuracy: 0.9343 - 58ms/epoch - 4ms/step\n",
            "Epoch 92/100\n",
            "14/14 - 0s - loss: 0.4323 - accuracy: 0.9319 - 50ms/epoch - 4ms/step\n",
            "Epoch 93/100\n",
            "14/14 - 0s - loss: 0.4231 - accuracy: 0.9413 - 47ms/epoch - 3ms/step\n",
            "Epoch 94/100\n",
            "14/14 - 0s - loss: 0.4126 - accuracy: 0.9437 - 48ms/epoch - 3ms/step\n",
            "Epoch 95/100\n",
            "14/14 - 0s - loss: 0.4153 - accuracy: 0.9437 - 48ms/epoch - 3ms/step\n",
            "Epoch 96/100\n",
            "14/14 - 0s - loss: 0.3903 - accuracy: 0.9507 - 49ms/epoch - 4ms/step\n",
            "Epoch 97/100\n",
            "14/14 - 0s - loss: 0.3794 - accuracy: 0.9531 - 49ms/epoch - 4ms/step\n",
            "Epoch 98/100\n",
            "14/14 - 0s - loss: 0.3639 - accuracy: 0.9531 - 47ms/epoch - 3ms/step\n",
            "Epoch 99/100\n",
            "14/14 - 0s - loss: 0.3515 - accuracy: 0.9601 - 49ms/epoch - 3ms/step\n",
            "Epoch 100/100\n",
            "14/14 - 0s - loss: 0.3495 - accuracy: 0.9601 - 47ms/epoch - 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbe14e3df10>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그 후 문자열을 주면 다음 문자를 계속해서 예측하는 것을 반복하여 최종적으로 문장을 완성시키는 함수를 생성"
      ],
      "metadata": {
        "id": "gFdtt_ReiLbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_generation(model, char_to_index, seq_length, seed_text, n):\n",
        "\n",
        "    # 초기 시퀀스\n",
        "    init_text = seed_text\n",
        "    sentence = ''\n",
        "\n",
        "    for _ in range(n):\n",
        "        encoded = [char_to_index[char] for char in seed_text] # 현재 시퀀스에 대한 정수 인코딩\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre') # 데이터에 대한 패딩\n",
        "        encoded = to_categorical(encoded, num_classes=len(char_to_index))\n",
        "\n",
        "        # 입력한 X(현재 시퀀스)에 대해서 y를 예측하고 y(예측한 글자)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "        \n",
        "        for char, index in char_to_index.items():\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 시퀀스 + 예측 글자를 현재 시퀀스로 변경\n",
        "        seed_text = seed_text + char\n",
        "\n",
        "        # 예측 글자를 문장에 저장\n",
        "        sentence = sentence + char\n",
        "\n",
        "    sentence = init_text + sentence\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "UMj5EvrcfoJZ"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력된 두 문장을 연속적으로 훈련 데이터에서 나온 적이 없는 문장임에도 모델이 임의로 생성해낸 것을 볼 수 있음"
      ],
      "metadata": {
        "id": "d2WHIXa3iM10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentence_generation(model, char_to_index, 10, 'I get on w', 80))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYwzYV3jfpKC",
        "outputId": "2307de57-30ee-47d0-8e22-cf6ebb352f1e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I get on with life as a programmer, I like to han wort Ialt to bee.. But whee I stop lo ta\n"
          ]
        }
      ]
    }
  ]
}