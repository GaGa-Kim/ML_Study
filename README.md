# 머신러닝 스터디 ⚙
> [머신러닝 스터디 기술 블로그](https://gaga-kim.tistory.com/category/STUDY/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5)
> - 혼자 공부하는 머신러닝 + 딥러닝
> - 딥 러닝을 이용한 자연어 처리
<br/>

## 혼자 공부하는 머신러닝 + 딥러닝
> 1. **나의 첫 머신러닝**<br/>
1-1. 인공지능과 머신러닝, 딥러닝<br/>
1-2. 코랩과 주피터 노트북<br/>
1-3. 마켓과 머신러닝<br/>
> 2. **데이터 다루기**<br/>
2-1. 훈련 세트와 테스트 세트<br/>
2-2. 데이터 전처리<br/>
> 3. **나의 첫 머신러닝**<br/>
3-1. k-최근접 이웃 회귀<br/>
3-2. 선형 회귀<br/>
3-3. 특성 공학과 규제<br/>
> 4. **다양한 분류 알고리즘**<br/>
4-1. 로지스틱 회귀<br/>
4-2. 확률적 경사 하강법<br/>
> 5. **트리 알고리즘**<br/>
5-1. 결정 트리<br/>
5-2. 교차 검증과 그리드 서치<br/>
5-3. 트리의 앙상블<br/>
> 6. **비지도 학습**<br/>
6-1. 군집 알고리즘<br/>
6-2. k-평균<br/>
6-3. 주성분 분석<br/>
> 7. **딥러닝을 시작합니다**<br/>
7-1. 인공 신경망<br/>
7-2. 심층 신경망<br/>
7-3. 신경망 모델 훈련<br/>
> 8. **이미지를 위한 인공 신경망**<br/>
8-1. 합성공 신경망의 구성 요소<br/>
8-2. 합성곱 신경망을 사용한 이미지 분류<br/>
8-3. 합성곱 신경망의 시각화<br/>
> 9. **텍스트를 위한 인공 신경망**<br/>
9-1. 순차 데이터와 순환 신경망<br/>
9-2. 순환 신경망으로 IMDB 리뷰 분류하기<br/>
9-3. LSTM과 GRU 셀<br/>
<br/>

## 딥 러닝을 이용한 자연어 처리
> 1. **자연어 처리 준비하기**<br/>
1-1. 자연어 처리<br/>
1-2. 필요 프레임워크, 라이브러리, 자연어 패키지<br/>
1-3. 판다스 프로파일링<br/>
1-4. 머신 러닝 워크플로우<br/>
> 2. **텍스트 전처리**<br/>
2-1. 텍스트 전처리<br/>
2-2. 토큰화<br/>
2-3. 정제와 정규화<br/>
2-4. 어간 추출과 표제어 추출<br/>
2-5. 불용어<br/>
2-6. 정규 표현식<br/>
2-7. 정수 인코딩<br/>
2-8. 패딩<br/>
2-9. 원-핫 인코딩<br/>
2-10. 데이터의 분리<br/>
2-11. 한국어 전처리 패키지<br/>
> 3. **언어 모델**<br/>
3-1. 언어 모델<br/>
3-2. 언어 모델이란?<br/>
3-3. 통계적 언어 모델<br/>
3-4. N-gram 언어 모델<br/>
3-5. 한국어에서의 언어 모델<br/>
3-6. 펄플렉서티<br/>
3-7. 조건부 확률<br/>
> 4. **카운트 기반의 단어 표현**<br/>
4-1. 카운트 기반의 단어 표현<br/>
4-2. 다양한 단어의 표현 방법<br/>
4-3. Bag of Words<br/>
4-4. 문서 단어 행렬<br/>
4-5. TF-IDF<br/>
> 5. **벡터의 유사도**<br/>
5-1. 벡터의 유사도<br/>
5-2. 코사인 유사도<br/>
5-3. 여러가지 유사도 기법<br/>
> 6. **머신 러닝 개요**<br/>
6-1. 머신 러닝이란<br/>
6-2. 머신 러닝 훑어보기<br/>
6-3. 선형 회귀<br/>
6-4. 로지스틱 회귀<br/>
6-5. 벡터와 행렬 연산<br/>
6-6. 소프트맥스 회귀<br/>
> 7. **딥 러닝 개요**<br/>
7-1. 딥 러닝 개요<br/>
7-2. 퍼셉트론<br/>
7-3. 인공 신경망 훑어보기<br/>
7-4. 행렬곱으로 이해하는 신경망<br/>
7-5. 딥 러닝의 학습 방법<br/>
7-6. 과적합을 막는 방법들<br/>
7-7. 기울기 소실과 폭주<br/>
7-8. 케라스 훑어보기<br/>
7-9. 케라스의 함수형 API<br/>
7-10. 케라스 서브클래싱 API<br/>
7-11. 다층 퍼셉트론으로 텍스트 분류하기<br/>
7-12. 피드 포워드 신경망 언어 모델<br/>
> 8. **순환 신경망**<br/>
8-1. 순환 신경망<br/>
8-2. 장단기 메모리<br/>
8-3. 게이트 순환 유닛<br/>
8-4. RNN 언어 모델<br/>
8-5. 문자 단위 RNN<br/>
